import torch
import pandas as pd
import numpy as np
import glob
import os
import gc
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, TensorDataset
from diffusers import UNet1DModel, DDPMScheduler
from torch.optim import AdamW
from sklearn.preprocessing import MinMaxScaler

# ==========================================
# 1. CONFIGURA√á√ïES VENCEDORAS (Do Optuna)
# ==========================================
# Baseado nos seus resultados: {'lr': 0.00097, 'batch_size': 16, 'base_channels': 32}
LEARNING_RATE = 9.7e-4
BATCH_SIZE = 16
BASE_CHANNELS = 32
SEQ_LENGTH = 64  # Tamanho da janela de tempo
EPOCHS = 500  # Treino longo para alta precis√£o
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
FEATURES = ['P-MON-CKP', 'T-JUS-CKP']  # Press√£o e Temperatura (as colunas √∫teis)

# ==========================================
# 2. GEST√ÉO DE MEM√ìRIA (Li√ß√£o do seu Projeto5E)
# ==========================================
if torch.cuda.is_available():
    torch.cuda.empty_cache()
gc.collect()
print(f"‚öôConfigurando treino na {DEVICE}...")


# ==========================================
# 3. CARREGAMENTO E TRATAMENTO
# ==========================================
def carregar_dados_final():
    print("Carregando dados finais...")
    # L√≥gica inteligente para achar a pasta 'data' onde quer que ela esteja
    diretorio_atual = os.path.dirname(os.path.abspath(__file__))
    path_padrao = os.path.join(diretorio_atual, "data", "raw", "*.parquet")
    path_pai = os.path.join(os.path.dirname(diretorio_atual), "data", "raw", "*.parquet")

    arquivos = glob.glob(path_padrao) + glob.glob(path_pai)

    dfs = []
    for arq in arquivos:
        try:
            df = pd.read_parquet(arq)
            cols = [c for c in FEATURES if c in df.columns]
            if not cols: continue

            # Filtra apenas dados de falha (Slugging) se houver coluna de classe
            if 'class' in df.columns:
                df = df[df['class'] != 0]

            if len(df) > 0: dfs.append(df[cols])
        except:
            pass

    if not dfs:
        raise ValueError("ERRO CR√çTICO: Nenhum dado de falha encontrado. Verifique a pasta 'data/raw'.")

    df_final = pd.concat(dfs, ignore_index=True)
    # Interpola√ß√£o para remover buracos nos dados (NaNs)
    df_final = df_final.interpolate(limit_direction='both').fillna(0)

    print(f"Dados carregados: {len(df_final)} pontos de falha real.")
    return df_final


# ==========================================
# 4. O MOTOR PRINCIPAL
# ==========================================
def main():
    # --- A. Prepara√ß√£o dos Dados ---
    df_real = carregar_dados_final()

    # Normaliza√ß√£o (-1 a 1 √© o padr√£o para Diffusion Models)
    # Guardamos o 'scaler' para desnormalizar no final (Li√ß√£o do seu Optuna_TCC_v2)
    scaler = MinMaxScaler(feature_range=(-1, 1))
    data_scaled = scaler.fit_transform(df_real.values)

    # Cria√ß√£o das Sequ√™ncias (Janelas Deslizantes)
    sequences = []
    step = SEQ_LENGTH // 2  # Overlap de 50%
    for i in range(0, len(data_scaled) - SEQ_LENGTH, step):
        sequences.append(data_scaled[i:i + SEQ_LENGTH])

    tensor_seq = torch.tensor(np.array(sequences), dtype=torch.float32).permute(0, 2, 1)
    dataset = TensorDataset(tensor_seq)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    print(f"Dataset formatado: {len(sequences)} sequ√™ncias de treino.")

    # --- B. Constru√ß√£o do Modelo (UNet) ---
    # Usando a arquitetura leve sugerida pelo Optuna (32 -> 64 -> 128)
    block_channels = (BASE_CHANNELS, BASE_CHANNELS * 2, BASE_CHANNELS * 4)

    model = UNet1DModel(
        sample_size=SEQ_LENGTH,
        in_channels=len(FEATURES),
        out_channels=len(FEATURES),
        layers_per_block=2,
        block_out_channels=block_channels,
        down_block_types=("DownBlock1D", "DownBlock1D", "AttnDownBlock1D"),
        up_block_types=("AttnUpBlock1D", "UpBlock1D", "UpBlock1D"),
    ).to(DEVICE)

    noise_scheduler = DDPMScheduler(num_train_timesteps=1000)
    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)

    # --- C. Loop de Treinamento ---
    print(f"\nIniciando Treinamento Final ({EPOCHS} √âpocas)...")
    model.train()
    loss_history = []

    try:
        for epoch in range(EPOCHS):
            epoch_loss = 0
            for batch in dataloader:
                clean_images = batch[0].to(DEVICE)
                noise = torch.randn_like(clean_images).to(DEVICE)
                timesteps = torch.randint(0, 1000, (clean_images.shape[0],), device=DEVICE).long()

                # Forward Process (Adiciona Ru√≠do)
                noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)

                # A Rede tenta prever o ru√≠do
                noise_pred = model(noisy_images, timesteps).sample

                loss = torch.nn.functional.mse_loss(noise_pred, noise)

                optimizer.zero_grad()
                loss.backward()
                # Gradient Clipping (Seguran√ßa para n√£o explodir)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()

                epoch_loss += loss.item()

            avg_loss = epoch_loss / len(dataloader)
            loss_history.append(avg_loss)

            if (epoch + 1) % 50 == 0:
                print(f"Epoch {epoch + 1}/{EPOCHS} | Loss: {avg_loss:.5f}")

    except KeyboardInterrupt:
        print("\nTreinamento interrompido manualmente. Salvando o progresso atual...")

    # Salvar o modelo treinado
    os.makedirs("models_final", exist_ok=True)
    model.save_pretrained("models_final/ddpm_slugging_v1")
    print("\nModelo Salvo em 'models_final/ddpm_slugging_v1'")

    # --- D. GERA√á√ÉO E DESNORMALIZA√á√ÉO ---
    print("üîÆ Gerando dados sint√©ticos (Inference)...")
    model.eval()

    # Come√ßamos com ru√≠do aleat√≥rio puro (distribui√ß√£o normal)
    noise = torch.randn(1, len(FEATURES), SEQ_LENGTH).to(DEVICE)

    # Processo de Denoising (Reverse Diffusion)
    for t in noise_scheduler.timesteps:
        with torch.no_grad():
            model_output = model(noise, t).sample
            noise = noise_scheduler.step(model_output, t, noise).prev_sample

    # Resultado gerado (ainda normalizado entre -1 e 1)
    generated_data_norm = noise.permute(0, 2, 1).cpu().numpy()[0]

    # DESNORMALIZA√á√ÉO (Voltando para Pascal e Celsius)
    generated_data_real = scaler.inverse_transform(generated_data_norm)

    # Pega um dado real para compara√ß√£o
    real_data_norm = sequences[0]
    real_data_real = scaler.inverse_transform(real_data_norm)

    # --- E. PLOTAGEM (Gr√°ficos Profissionais) ---
    plt.figure(figsize=(12, 8))

    # Subplot 1: Real
    plt.subplot(2, 1, 1)
    plt.plot(real_data_real[:, 0], label='Press√£o Real (Pa)', color='blue')
    plt.title("DADO REAL (Original Petrobras 3W)")
    plt.ylabel("Press√£o (Pa)")
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Subplot 2: Sint√©tico
    plt.subplot(2, 1, 2)
    plt.plot(generated_data_real[:, 0], label='Press√£o Sint√©tica (Gerada)', color='red', linestyle='--')
    plt.title("DADO SINT√âTICO (Criado pelo Diffusion Model)")
    plt.ylabel("Press√£o (Pa)")
    plt.xlabel("Tempo (Passos)")
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # Gr√°fico de Loss
    plt.figure(figsize=(8, 4))
    plt.plot(loss_history)
    plt.title("Evolu√ß√£o do Erro (Loss)")
    plt.xlabel("√âpocas")
    plt.ylabel("MSE Loss")
    plt.grid(True)
    plt.show()


if __name__ == "__main__":
    main()